<!DOCTYPE NoteCase-File>
<!--LastNote:10-->
<HTML>
<HEAD>
<meta content="text/html;charset=UTF-8" http-equiv="Content-Type">
<meta name="generator" content="NoteCase 1.9.8">
<TITLE></TITLE>
</HEAD>
<BODY>
<DL>
<DT>大数据</DT>
<!--property:date_created=1471942099-->
<!--property:date_modified=1471971538-->
<!--property:expanded-->
<DD>
<DL>
<DT>ETL</DT>
<!--property:date_created=1471942250-->
<!--property:date_modified=1472137119-->
<!--property:expanded-->
<DD>
ETL，是英文 Extract-Transform-Load 的缩写，用来描述将数据从来源端经过抽取（extract）、转换（transform）、加载（load）至目的端的过程。<BR>
<BR>
抽取：简单而繁琐（各种数据源【不规范的数据转为格式化的数据】【界面，库】）<BR>
转换：操作的是格式化后的数据（对数据进行一些操作【移动数据、根据规则验证数据、数据内容和数据结构的修改、将多个数据源的数据集成】）<BR>
加载：根据数据库的不同用不同的方式加载<BR>
<BR>
ETL一词较常用在数据仓库，但其对象并不限于数据仓库。<BR>
ETL是指从源系统中提取数据，转换数据为一个标准的格式，并加载数据到目标数据存储区，通常是数据仓库。<BR>
<BR>
ETL实现方式：<BR>
	1）手工编码，编写脚本，Java，Python<BR>
	2）商业ETL工具软件  （informatica、 IBM DataStage 、Microsoft SSIS 、 Oracle ODI）<BR>
	3）开源ETL工具软件  （Kettle 、 Talend 、 CloverETL）<BR>
	<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>

</DD>
</DL>

</DD>
<DT>Kettle</DT>
<!--property:date_created=1471942716-->
<!--property:date_modified=1473387110-->
<!--property:expanded-->
<DD>
1.介绍：	Kettle是一款国外开源的ETL工具，纯java编写。             （作者：Matt）<BR>
		该项目的程序员希望把各种数据放到一个壶里，然后以一种指定的格式流出。<BR>
<BR>
2.优点：1）插件架构，扩展性好<BR>
	   2）全面的数据访问支持<BR>
	   3）支持多平台（Java开发）<BR>
	   4）多种方式应用集成（可以写jar包加入到kettle中去，或者在Web项目中调用kettle）<BR>
	   5）全面优化高效稳定（一个操作一个线程，或者一个操作多个线程）<BR>
<BR>
Kettle这个ETL工具集，它允许你管理来自不同数据库的数据，通过提供一个图形化的用户环境来描述你想做什么，而不是你想怎么做。<BR>
<BR>
Kettle家族目前包括4个产品：Spoon、Pan、CHEF、Kitchen。<BR>
<BR>
SPOON 允许你通过图形界面来设计ETL转换过程（Transformation）。<BR>
<BR>
PAN 允许你批量运行由Spoon设计的ETL转换 (例如使用一个时间调度器)。Pan是一个后台执行的程序，没有图形界面。<BR>
<BR>
CHEF 允许你创建任务（Job）。 任务通过允许每个转换，任务，脚本等等，更有利于自动化更新数据仓库的复杂工作。<BR>
任务通过允许每个转换，任务，脚本等等。任务将会被检查，看看是否正确地运行了。<BR>
<BR>
KITCHEN 允许你批量使用由Chef设计的任务 (例如使用一个时间调度器)。KITCHEN也是一个后台运行的程序。 <BR>
<BR>
<span style="color:#ff0000"><b>数据清洗</b></span>是指发现并纠正数据文件中可识别的错误的最后一道程序，包括检查数据一致性，处理无效值和缺失值等。<BR>
<BR>
<BR>
Kettle下载安装：要有JRE环境、连接资源库需要JDBC的jar包(放在kettle某个目录下)<BR>
<BR>
 	①：  如果启动还报错“could not create the java virtual machine”，不是java虚拟机出了问题，修改一下spoon.bat里内存配置<BR>
      			   if &quot;%PENTAHO_DI_JAVA_OPTIONS%&quot;==&quot;&quot; set PENTAHO_DI_JAVA_OPTIONS=&quot;-Xms2058m&quot; &quot;-Xmx1024m&quot; &quot;-XX:MaxPermSize=256m&quot;<BR>
      		     改为<BR>
     		    if &quot;%PENTAHO_DI_JAVA_OPTIONS%&quot;==&quot;&quot; set PENTAHO_DI_JAVA_OPTIONS=&quot;-Xms512m&quot; &quot;-Xmx512m&quot; &quot;-XX:MaxPermSize=256m&quot;<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<DL>
<DT>Kettle的使用</DT>
<!--property:date_created=1471969917-->
<!--property:date_modified=1472314990-->
<!--property:expanded-->
<DD>
1.transformation（转换）<BR>
<BR>
2.job（工作）<BR>
<BR>
<BR>
3.Kettle资源库：<BR>
		1）用Kettle进行ETL开发的几个阶段：开发、测试、确认、发布<BR>
		2）各阶段对应的资源库：开发资源库、测试（确认）资源库、发布资源	<BR>
		3）各阶段推进：--从开发资源库到测试资源库：①注意命名规则。<BR>
										     	        ②两种移植方法：断开/重连（文件少时使用），导出/导入		<BR>
	        4）数据库资源参数化（参考 JDBCTool）：因为各个阶段的环境不一样，数据库连接等元数据不能硬编码<BR>
							参数化的几种方法：①<BR>
											  ②<BR>
	 										  ③			<BR>
				<BR>
4.Kettle运行方式：1）图形界面：Spoon（开发）<BR>
					① 本地：在本地执行<BR>
					② 远程：在远程服务器执行，需要远程服务器执行Carte。<BR>
							· Carte 是内嵌的Jetty 的http server<BR>
							· Carte 执行命令 carte localhost 8080<BR>
					③ 集群：在集群上执行（多台机器），需要转换里的某个步骤事先设置为集群方式运行。<BR>
												（要有一台是master 节点，主服务器）<BR>
				 2）命令行：Pan （执行转换）、 Kitchen （执行作业）<BR>
				 3）API： Kettle JavaAPI 嵌入到其他应用<BR>
<BR>
4--1）--①--：远程执行步骤：启动carte服务（carte localhost 9111【不与本机的端口号重复】）--- 在子服务器中建立服务器（设置远程服务器）<BR>
 <BR>
4--2）：命令行的参数格式：①/参数名：值<BR>
						 ②-参数名=值<BR>
						例子1.执行test.ktr 文件 日志保存在D:/log.txt中，日志级别是Rowlevel <BR>
						Pan /file:D:\AppProjects\nxkh\test.ktr   /logfile:D:\log.txt    /level:Rowlevel<BR>
						<BR>
5.执行结果：1）激活（状态）：初始化、运行、已完成、挂起<BR>
		      2）Print/out：步骤之间的缓存（buffer）【每个步骤之间都有缓存】	<BR>
															<BR>
6.执行步骤：每个步骤之间都有一个buffer（缓存），上一个模块往buffer里写，下一个模块从buffer里读数据。<BR>
<BR>
7.日志：1）日志参数设置： 工具-选项  （最大行数、保留时间）<BR>
	      2）转换的四个日志表【设置-日志】：（转换【合在一起】、步骤【分开每个步骤】、性能、日志通道）<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
 <BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
	<DL>
<DT>转换</DT>
<!--property:date_created=1472014830-->
<!--property:date_modified=1472430730-->
<!--property:expanded-->
<DD>
1.转换类步骤：是对数据进行各种形式转换所用到的步骤（字段选择、计算机、增加常量）<BR>
<BR>
2.流程步骤：是用来控制数据流的步骤。一般不对数据进行操作，只是控制数据流（过滤）<BR>
<BR>
3.连接：结果集通过关键字进行连接（记录集连接、记录关联【笛卡尔输出{X x X   ，2x3=6条记录}】）<BR>
		<BR>
4.查询步骤：用来查询数据源里的数据并合并到主数据流中。<BR>
				<BR>
		<BR>
				<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<DL>
<DT>输入</DT>
<!--property:date_created=1472315665-->
<!--property:date_modified=1473489813-->
<DD>
1.生成记录（生成一样的数据）/自定义常量（自定义值，比较灵活）<BR>
<BR>
2.获取系统信息：获取各类系统信息，常见的包括（转换开始时间、关键时间点信息、）<BR>
									【不同的信息类型】熟悉<BR>
				①命令行参数：运行转换时，输入对应的值<BR>
<BR>
3.表输入：1）可以通过？和${var}方式使用变量，          ？变量要求前面步骤传来的参数的顺序一致<BR>
		  2）允许简易转换（延迟转换）这样从数据库读取的数据就能保持原有字符集，通过字节传输（不转换为String）<BR>
		3）执行每一行：使用变量后，传入多个数据，每一个数据执行一次sql语句（减少与数据库的连接次数，增加效率）<BR>
<BR>
4.处理文本文件输入：处理有列分隔符（限定符、逃逸字符）的文本文件。<BR>
				1）文件选择方式：① 直接选择本地文件<BR>
								② 从上个步骤传递文件名<BR>
				2）内容：文本限定符（”）、分隔符（；）的设置，不可见字符输入。行头设置。   逃逸字符(\)<BR>
				3）Kettle 中变量  ${Internal.Transformation.Filename.Directory}，是指你保存这个转换的目录；<BR>
<BR>
5.XML文件输入<BR>
<BR>
6.Json输入
</DD>
<DT>输出</DT>
<!--property:date_created=1472316987-->
<!--property:date_modified=1473490341-->
<DD>
1.表输出：使用SQL方式向数据库提交数据<BR>
			1）支持批量提交<BR>
			2）支持数据分区（要求分区字段是Date型）。   （半自动，要自行建表【建好表，Kettle再把数据塞进去】）<BR>
			3）字段映射（与字段选择步骤的区别）。<BR>
			4）返回自增列。	<BR>
			5）自动生成关键字，把字段加入到流中去（不管有没有输出）<BR>
			6）数据库字段：① 表里的字段指你从表输入或表输出中读取到的数据库中表的字段<BR>
						    ②  流字段指的是你kettle流程中输入控件输出的字段<BR>
						    ③  <span style="color:#ff0000">更新操作，要清除数据库缓存（数据库字段-获取字段）<BR>
						   </span> <span style="color:#000000">④ 过滤字段后，流里的字段不会减少（其他过滤操作，过滤流中的字段）</span><BR>
<BR>
2.SQL文件输出：输出数据库的建表语句<BR>
<BR>
3.文本文件输出：<BR>
<BR>
4.XML输出<BR>
<BR>
5.Excel 输出：<BR>
<BR>
6.数据库操作：1）删除：匹配上了删除<BR>
			  2）更新：匹配上了，更新（更新字段里面的字段）<BR>
			  3）插入/更新：匹配上了，去数据库里匹配每一条记录，没找到就插入（可以不更新，只插入）<BR>
			<BR>
7.数据同步： 1）合并记录（从旧数据源到新数据源发生的改变）<BR>
					① 要求行的列数相同			<BR>
					② 匹配的关键字：关键字段（主键）【关键字段对上了，数据字段不同则为changed】<BR>
						数据字段（要对比的值）<BR>
										<BR>
		       2） 数据同步（<span style="color:#ff0000">？</span>）：与插入/更新类似，加了操作字段名<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>

</DD>
<DT>转换</DT>
<!--property:date_created=1472352457-->
<!--property:date_modified=1473493342-->
<DD>
1.增加列：1） 计算器：增加列<BR>
<BR>
2.字符串处理：拆分：1）剪切字符串（按位置拆分）：0到1，剪切第一个字母<BR>
		  	 2） 拆分字段（按<span style="color:#ff0000"><b><u>标志字符</u></b></span>一列拆分为多列）：<BR>
		  	 3） 列拆分为多行（按标志字符一列拆分为多行） ： （正则表达？）<BR>
		  			    【a;a;a；a<BR>
						a;a;a；a<BR>
						a;a;a；a】<BR>
		 	4）字符串替换：（匹配上的部分被替换，可以用正则表达式）<BR>
			5）值映射：替换值		<BR>
			6）字符串操作：<BR>
							<BR>
3.行列变换： 1）列转行<BR>
			<BR>
	           2）行转列<BR>
<BR>
4.排序/排重/字段选择：1）排序记录<BR>
<BR>
			  	        2）排重：① 去除重复记录（需要先进行排序）<BR>
				       		         ② 唯一行（哈希值）【不需要事先排序，占内存】（计算哈希值，存放在内存中进行比较）<BR>
			    		<BR>
					3）字段选择：① 修改字段名称<BR>
								 ② 删除字段<BR>
								 ③ 修改字段类型<BR>
								<BR>
					4）设置字段值：用其他字段值替换当前字段值			<BR>
<BR>
					5）将字段值设置为常量：用常量代替【选中字段的】字段值<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>

</DD>
<DT>流程、应用</DT>
<!--property:date_created=1472367227-->
<!--property:date_modified=1472399219-->
<DD>
1.过滤数据： 1）Switch/Case : 多路开关，一路到多路<BR>
				支持日期、数值、字符串类型比较。<BR>
				<BR>
		      2）过滤记录： 多路开关，一路到两路<BR>
				支持日期、数值、字符串类型比较，自定义可嵌套的表达式。<BR>
					（把符合条件的数据传送过去）<BR>
				<BR>
		     3）根据Java表达式过滤记录： 多路开关，一路到两路<BR>
				直接手写 Java 表达式，比过滤记录更灵活，尤其在字符串的处理上。	<BR>
<BR>
2. 处理不确定的数据行数（设置变量）：1）检测空流： 有则不通过，没有则生成一个空行<BR>
<BR>
								    2）识别流的最后一行： 通过一个字段，标识出数据流里的最后一行 <BR>
<BR>
								    3）阻塞数据： 除了最后一条，其他的数据行都不能过去<BR>
<BR>
3. 多来源数据行的合并： （要求列名、列数、列类型相同）<BR>
					  1）空操作： 多个来源，以自然顺序合并（不能保证顺序）<BR>
					  2）追加流： 只能两个来源，指定顺序合并<BR>
					  3）数据流优先级排序 ：多个来源，指定顺序合并<BR>
<BR>
4.数据的流程的终点：除了各个输出步骤，还包括下面几个<BR>
					1）空操作 ： 垃圾箱<BR>
					2）终止 ：转换强行停止，并报错误<BR>
					3）复制记录到结果 ： 暂时保留在内存中，供以后的转换使用<BR>
					4）设置变量（只能留一个值）：<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
										
</DD>
<DT>查询、连接</DT>
<!--property:date_created=1472431389-->
<!--property:date_modified=1473663815-->
<DD>
1.流查询：要有一个字典源和一个数据源（匹配上了后，可选把字典源数据加入到数据源中去）<BR>
			1）匹配上多条，只保留最后一条<BR>
			2）只支持等于查询。<BR>
			<BR>
				<BR>
2.				
</DD>
</DL>

</DD>
<DT>资源库</DT>
<!--property:date_created=1472114435-->
<!--property:date_modified=1473478108-->
<DD>
数据库   连接 、库、表<BR>
1.数据库资源库：利用kettle中的“资源库”，会把元数据统一存储在数据库中，这样利于个人对于本库ktr的统一管理及更改。<BR>
			（表的形式）<BR>
			1）资源库建立的问题：sql语句执行错误（mysql不能插入Boolean型的数据）<BR>
												（boolean改成Char（1））   【（改成 '0' 或者 '1'）？】<BR>
												<BR>
			2）资源库登录：用户名和密码在r_user表内，需要加密才能使用（例：1  admin	2be98afc86aa7f2e4cb79ce71da9fa6d4<BR>
								（加密后的密码，相当于admin） 	Administrator	User manager	1）	<BR>
					<BR>
			3）资源文件夹：r_transformation	（存放文件信息）	 r_job（作业信息）<BR>
			<BR>
			4）修改资源库密码，探索资源库-安全			<BR>
<BR>
2.文件资源库：在文件的基础上的封装，实现了org.pentaho.di.repository.Repository接口。<BR>
							<BR>
3.不使用资源库							<BR>
		<BR>
4.资源库的导出导入：	    导出的为XML文件，包含所有的作业和转换<BR>
				1）全部导出<BR>
				2）导出一个目录													<BR>
							<BR>
							<BR>
							<BR>
							<BR>
							<BR>
							<BR>
							<BR>
							<BR>
							<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
							
</DD>
<DT>脚本步骤</DT>
<!--property:date_created=1472135532-->
<!--property:date_modified=1472180387-->
<DD>
数据剖析和数据检验: 用于数据的检查、清洗。<BR>
			1）数据剖析：分析并统计出原数据的类型、取值范围、长度、最大最小值等。<BR>
			2）数据剖析是ETL工作的第一步，在ETL的需求阶段就要开始。<BR>
			3）数据剖析的结果，是将来数据检验、更正步骤的依据。<BR>
			4）Kettle里使用的数据剖析工具是DataCleaner。<BR>
<BR>
统计步骤： 提供数据采样和统计的功能<BR>
<BR>
分区：根据数据里某个字段的值，拆分成多个数据块。输出到不同的库表和文件中。（表输出中的Date参数分区）<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>

</DD>
</DL>

</DD>
<DT>练习</DT>
<!--property:date_created=1472021480-->
<!--property:date_modified=1472021650-->
<DD>
1.生成 100 个随机数，随机数取值于【0，100）之间，计算小于等于 50 的随机数个数和 大于50 的随机数个数。<BR>
   并把这两个统计数字放在数据库表的一行的两列中，即输出的结果有一行， 一行包括两列， 每列是一个统计值。<BR>
  
</DD>
</DL>

</DD>
<DT>SQL</DT>
<!--property:date_created=1472520886-->
<!--property:date_modified=1472520914-->
<DD>
1. CREATE TABLE if not exists t_stu_err ( 创建表前判断表是否存在 )
</DD>
<DT>正则表达式</DT>
<!--property:date_created=1472091453-->
<!--property:date_modified=1472091918-->
<DD>
代码	说明<BR>
.	匹配除换行符以外的任意字符<BR>
\w	匹配字母或数字或下划线或汉字<BR>
\s	匹配任意的空白符<BR>
\d	匹配数字<BR>
\b	匹配单词的开始或结束<BR>
^	匹配字符串的开始<BR>
$	匹配字符串的结束                  (匹配行的开始和结束)<BR>
*    前面任意的数量  (可能是0次)<BR>
\d+  匹配一个或更多连续的数字(重复一次或更多次)<BR>
<BR>
\转义字符<BR>
<BR>
<BR>
表2.常用的限定符<BR>
代码/语法	说明<BR>
*	重复零次或更多次<BR>
+	重复一次或更多次<BR>
?	重复零次或一次<BR>
{n}	重复n次<BR>
{n,}	重复n次或更多次<BR>
{n,m}	重复n到m次
</DD>
<DT>大数据从无到有：知乎</DT>
<!--property:date_created=1472175552-->
<!--property:date_modified=1472177105-->
<DD>
所以，题主问如何创建一个大数据平台，那么我觉得第一个步骤绝对是做好定位。<BR>
而对产品的定义，往往都是需求决定的，所以先问问自己或者领导们，为什么我们要做大数据平台？<BR>
确定有这个必要么？你们真的需要一个完整的大数据平台，还是只需要一个能够方便进行并行计算的系统？<BR>
这一步的定位直接影响到后续工作的展开以及各种成本（人力、资金、时间），也关系到开展难度及最终收益。<BR>
<BR>
做技术，尤其是没有太多经验从零开始做的时候，经常会为了做技术而做技术，这实际上是不可取的，我个人也在这点上栽过很多次。<BR>
所以我的建议是，这一步请千万不要任性。<BR>
<BR>
当对要做的产品有个很好的定位的时候，对一些概念也有了基本的认识，那么这个时候就要开始涉及技术选型啦。<BR>
我个人的建议是由最贴近用户的那个组件由上往下开始逐一推导选择。加入你们大数据平台的用户是一群直接使用SQL的BI，<BR>
那么这个时候用户接口那一层技术要么HIVE要么Spark sql，然后结合你们物理设备的状态，成本或者技术倾向性，技术背景等等因素做出选择。<BR>
<BR>
由于现在社区活跃度高，网络上有很多例子可以参考，所以对于初期的架构搭建难度并不会很大，只要找个例子先把几个组件搭建起来<BR>
就是第一步了。比如选择spark做计算引擎，选择hdfs做分布式文件系统，如果应用需要加个hbase，有了hbase必定会有zookeeper，<BR>
如果要考虑流式计算和消息总线那么就加个kafka，然后用spark streaming做流式计算引擎等等等等。这一步只要设备到位，<BR>
应该可以快速实现的。<BR>
<BR>
为什么我会认为这一步应当快速实现？我非常推崇一句话就是“好的架构不是设计出来的而是演化出来的”。当你有了系统主体以后，<BR>
你的系统首先是一个可运行的系统，这个系统已经可以简单地被用户开始使用了，比如跑跑简单的数据分析等等。<BR>
然而在使用的过程中必定会发生很多很多事情，比如应用运行过程中性能低，哦，那么我们开始调整系统参数进行优化；<BR>
比如应用运行过程中经常崩溃，哦，我先排个雷然后加个ganglia把系统监视起来……对于在生产环境中大规模使用的大数据平台这样的系统来<BR>
说，相信我，不会有一个放之四海皆准的方案，具体怎么实施，一些参数怎么配置，都需要不停地演化。<BR>
而系统的演化并不是架构师或者老板拍脑袋得到的，应当是由业务驱动的，是由实际出现的问题驱动的，这个过程是需要研发团队全程介入的。<BR>
<BR>
上面的是关于技术的，然而一个大数据平台这样的系统，它的运行成功或失败绝不是仅仅取决于技术，它同样涉及到对人和制度的管理。<BR>
建立一个平台就是在创造一套规则，发明一个游戏，然后让其他人在你的游戏规则下进行游戏。游戏规则是否科学会直接影响到这个平台执<BR>
行的好坏。关于这点，可以参考一些成功的公有数据平台的方案，进行简化，本地化，然后拉上线。当然，这套规则也是在不断演化的。<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>

</DD>
<DT>分布式文件系统</DT>
<!--property:date_created=1472177498-->
<!--property:date_modified=1472177563-->
<!--property:expanded-->
<DD>
分布式文件系统:   可以有效解决数据的存储和管理难题<BR>
	将固定于某个地点的某个文件系统，扩展到任意多个地点/多个文件系统，<BR>
众多的节点组成一个文件系统网络。每个节点可以分布在不同的地点，通过网络进行节点间的通信和数据传输。人们在使用分布式<BR>
文件系统时，无需关心数据是存储在哪个节点上、或者是从哪个节点从获取的，只需要像使用本地文件系统一样管理和存储文件系<BR>
统中的数据。
</DD>
<DT>BI</DT>
<!--property:date_created=1472177641-->
<!--property:date_modified=1472177766-->
<DD>
报表系统： Cognos、OBIEE、BusinessObjects<BR>
ETL工具： Datastage、Informatica、Kettle<BR>
BI技术： Perl、Shell、SQL<BR>
<BR>
<BR>
kettle只是ETL工具的一种，BI工具包括数据建模工具、ETL工具、OLAP数据分析工具、数据挖掘工具、数据可视化展现工具等等。<BR>
好多厂商的BI套件包括上述工具全部或部分套件.<BR>
<BR>
BI作用域在广，从数据库→数据仓库→数据挖掘→数据展示等，都有工具，kettle可以算是数据库到数据仓库使用工具的一种.<BR>
<BR>
数据挖掘对数据进行深度的处理<BR>
<BR>
要将庞大的数据转换成为有用的信息，必须先有效率地收集信息。随着科技的进步，功能完善的数据库系统就成了最好的收集数据的工具。<BR>
数据仓库，简单地说，就是搜集来自其它系统的有用数据，存放在一整合的储存区内。所以其实就是一个经过处理整合，且容量特别大的<BR>
关系型数据库，用以储存决策支持系统（Decision Support System）所需的数据，供决策支持或数据分析使用。从信息技术的角度来看<BR>
，数据仓库的目标是在组织中，在正确的时间，将正确的数据交给正确的人。
</DD>
</DL>
</BODY>
</HTML>
